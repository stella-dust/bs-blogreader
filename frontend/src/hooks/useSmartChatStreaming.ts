import { useState, useCallback, useRef } from 'react'
import { useChatSettingsStore } from '@/stores/chatSettingsStore'
import { useMetricsStore } from '@/stores'
import { SmartChatProcessor, createSmartChatProcessor } from '@/utils/smartChatProcessor'
import type { EnhancedChatMessage, LLMConfig, ProcessMode } from '@/stores/types'

interface UseSmartChatStreamingOptions {
  apiBaseUrl: string
  llmConfig: LLMConfig
  blogId?: string
  blogContent?: string
  onMessageStart?: (mode: ProcessMode['type']) => void
  onMessageChunk?: (chunk: string, mode: ProcessMode['type']) => void
  onMessageComplete?: (message: EnhancedChatMessage) => void
  onError?: (error: Error, mode: ProcessMode['type']) => void
}

export function useSmartChatStreaming({
  apiBaseUrl,
  llmConfig,
  blogId,
  blogContent,
  onMessageStart,
  onMessageChunk,
  onMessageComplete,
  onError
}: UseSmartChatStreamingOptions) {
  const [isProcessing, setIsProcessing] = useState(false)
  const [currentMode, setCurrentMode] = useState<ProcessMode['type'] | null>(null)
  const [streamingContent, setStreamingContent] = useState('')
  const [error, setError] = useState<Error | null>(null)

  const abortControllerRef = useRef<AbortController | null>(null)
  const processorRef = useRef<SmartChatProcessor | null>(null)
  const { settings } = useChatSettingsStore()
  const { startProcess, endProcess } = useMetricsStore()

  // åˆå§‹åŒ–å¤„ç†å™¨
  const initializeProcessor = useCallback(() => {
    if (!processorRef.current ||
        processorRef.current['apiBaseUrl'] !== apiBaseUrl ||
        processorRef.current['llmConfig'] !== llmConfig) {
      processorRef.current = createSmartChatProcessor(apiBaseUrl, llmConfig)
    }
    return processorRef.current
  }, [apiBaseUrl, llmConfig])

  /**
   * æµå¼å¤„ç†èŠå¤©æ¶ˆæ¯
   */
  const processMessage = useCallback(async (input: string) => {
    try {
      setIsProcessing(true)
      setStreamingContent('')
      setError(null)

      const processor = initializeProcessor()
      abortControllerRef.current = new AbortController()

      // é¢„æµ‹å¤„ç†æ¨¡å¼
      const prediction = SmartChatProcessor.predictMode(input, settings)
      setCurrentMode(prediction.mode.type)
      onMessageStart?.(prediction.mode.type)

      console.log(`ğŸ¤– æ™ºèƒ½å¤„ç†æ¨¡å¼: ${prediction.mode.type}`, prediction)

      // å¼€å§‹å¤„ç†è®¡é‡
      startProcess('chat')

      // æ ¹æ®æ¨¡å¼é€‰æ‹©æµå¼å¤„ç†æ–¹æ³•
      switch (prediction.mode.type) {
        case 'url_fetch':
          await processUrlFetchStreaming(input, processor, prediction.mode.type)
          break
        case 'web_search':
          await processWebSearchStreaming(input, processor, prediction.mode.type)
          break
        case 'rag_only':
          await processRAGStreaming(input, processor, prediction.mode.type)
          break
        default:
          throw new Error(`Unknown processing mode: ${prediction.mode.type}`)
      }

    } catch (err) {
      const error = err instanceof Error ? err : new Error('Unknown error')
      console.error('Smart chat streaming error:', error)

      if (error.name !== 'AbortError') {
        setError(error)
        onError?.(error, currentMode || 'rag_only')
        endProcess('chat', { progress: 0 })
      }
    } finally {
      setIsProcessing(false)
      setCurrentMode(null)
      abortControllerRef.current = null
    }
  }, [settings, initializeProcessor, onMessageStart, onError, startProcess, endProcess, currentMode])

  /**
   * URLæŠ“å–æ¨¡å¼æµå¼å¤„ç†
   */
  const processUrlFetchStreaming = useCallback(async (
    input: string,
    processor: SmartChatProcessor,
    mode: ProcessMode['type']
  ) => {
    // URLæŠ“å–æš‚æ—¶éæµå¼ï¼Œä½†å¯ä»¥åˆ†æ­¥éª¤æ˜¾ç¤ºè¿›åº¦
    setStreamingContent('ğŸ”— æ­£åœ¨æŠ“å–ç½‘é¡µå†…å®¹...')
    onMessageChunk?.('ğŸ”— æ­£åœ¨æŠ“å–ç½‘é¡µå†…å®¹...', mode)

    const result = await processor.process(input, blogId || '', settings, blogContent)

    if (result.success && result.message) {
      // æ¨¡æ‹Ÿæµå¼è¾“å‡ºæ•ˆæœ
      const content = result.message.content
      await simulateStreamingOutput(content, mode)

      endProcess('chat')
      onMessageComplete?.(result.message)
    } else {
      throw new Error(result.error || 'URLæŠ“å–å¤±è´¥')
    }
  }, [blogId, blogContent, settings, onMessageChunk, onMessageComplete, endProcess])

  /**
   * Webæœç´¢æ¨¡å¼æµå¼å¤„ç†
   */
  const processWebSearchStreaming = useCallback(async (
    input: string,
    processor: SmartChatProcessor,
    mode: ProcessMode['type']
  ) => {
    // åˆ†é˜¶æ®µæ˜¾ç¤ºæœç´¢è¿›åº¦
    const stages = [
      'ğŸ” æ­£åœ¨æœç´¢ç›¸å…³ä¿¡æ¯...',
      'ğŸŒ æ­£åœ¨æŠ“å–ç½‘é¡µå†…å®¹...',
      'ğŸ§  æ­£åœ¨åˆ†æå’Œæ•´åˆä¿¡æ¯...',
      'âœï¸ æ­£åœ¨ç”Ÿæˆç»¼åˆå›ç­”...'
    ]

    for (let i = 0; i < stages.length; i++) {
      if (abortControllerRef.current?.signal.aborted) return

      const stageText = `${stages[i]}\n\n${'â–“'.repeat(i + 1)}${'â–‘'.repeat(stages.length - i - 1)} ${Math.round((i + 1) / stages.length * 100)}%`
      setStreamingContent(stageText)
      onMessageChunk?.(stageText, mode)

      await new Promise(resolve => setTimeout(resolve, 800)) // æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
    }

    // å®é™…å¤„ç†
    const result = await processor.process(input, blogId || '', settings, blogContent)

    if (result.success && result.message) {
      // æ¸…ç©ºè¿›åº¦ï¼Œå¼€å§‹çœŸæ­£çš„å†…å®¹æµå¼è¾“å‡º
      await simulateStreamingOutput(result.message.content, mode)

      endProcess('chat')
      onMessageComplete?.(result.message)
    } else {
      throw new Error(result.error || 'Webæœç´¢å¤±è´¥')
    }
  }, [blogId, blogContent, settings, onMessageChunk, onMessageComplete, endProcess])

  /**
   * RAGæ¨¡å¼æµå¼å¤„ç†
   */
  const processRAGStreaming = useCallback(async (
    input: string,
    processor: SmartChatProcessor,
    mode: ProcessMode['type']
  ) => {
    // RAGæ¨¡å¼æ˜¾ç¤ºæ£€ç´¢è¿›åº¦
    setStreamingContent('ğŸ“š æ­£åœ¨æ£€ç´¢ç›¸å…³å†…å®¹...')
    onMessageChunk?.('ğŸ“š æ­£åœ¨æ£€ç´¢ç›¸å…³å†…å®¹...', mode)

    await new Promise(resolve => setTimeout(resolve, 500))

    setStreamingContent('ğŸ“š æ­£åœ¨æ£€ç´¢ç›¸å…³å†…å®¹...\nğŸ§  æ­£åœ¨ç†è§£å’Œåˆ†æ...')
    onMessageChunk?.('ğŸ“š æ­£åœ¨æ£€ç´¢ç›¸å…³å†…å®¹...\nğŸ§  æ­£åœ¨ç†è§£å’Œåˆ†æ...', mode)

    const result = await processor.process(input, blogId || '', settings, blogContent)

    if (result.success && result.message) {
      await simulateStreamingOutput(result.message.content, mode)

      endProcess('chat')
      onMessageComplete?.(result.message)
    } else {
      throw new Error(result.error || 'RAGå¤„ç†å¤±è´¥')
    }
  }, [blogId, blogContent, settings, onMessageChunk, onMessageComplete, endProcess])

  /**
   * æ¨¡æ‹Ÿæµå¼è¾“å‡ºæ•ˆæœ
   */
  const simulateStreamingOutput = useCallback(async (content: string, mode: ProcessMode['type']) => {
    const words = content.split('')
    let accumulated = ''

    for (let i = 0; i < words.length; i++) {
      if (abortControllerRef.current?.signal.aborted) return

      accumulated += words[i]
      setStreamingContent(accumulated)
      onMessageChunk?.(accumulated, mode)

      // åŠ¨æ€å»¶è¿Ÿï¼šæ ‡ç‚¹ç¬¦å·åç¨å¾®åœé¡¿ï¼Œå…¶ä»–å­—ç¬¦å¿«é€Ÿè¾“å‡º
      const char = words[i]
      const delay = /[ã€‚ï¼ï¼Ÿ\n]/.test(char) ? 100 :
                   /[ï¼Œã€ï¼›ï¼š]/.test(char) ? 50 :
                   /\s/.test(char) ? 30 : 15

      await new Promise(resolve => setTimeout(resolve, delay))
    }
  }, [onMessageChunk])

  /**
   * åœæ­¢å¤„ç†
   */
  const stopProcessing = useCallback(() => {
    if (abortControllerRef.current) {
      abortControllerRef.current.abort()
    }
    setIsProcessing(false)
    setCurrentMode(null)
  }, [])

  /**
   * é‡ç½®çŠ¶æ€
   */
  const reset = useCallback(() => {
    setStreamingContent('')
    setError(null)
    setIsProcessing(false)
    setCurrentMode(null)
  }, [])

  /**
   * æ›´æ–°LLMé…ç½®
   */
  const updateLLMConfig = useCallback((newConfig: LLMConfig) => {
    if (processorRef.current) {
      processorRef.current.updateLLMConfig(newConfig)
    }
  }, [])

  return {
    // çŠ¶æ€
    isProcessing,
    currentMode,
    streamingContent,
    error,

    // æ–¹æ³•
    processMessage,
    stopProcessing,
    reset,
    updateLLMConfig,

    // å·¥å…·æ–¹æ³•
    predictMode: useCallback((input: string) =>
      SmartChatProcessor.predictMode(input, settings), [settings])
  }
}